version: "3.9"

services:
  # ---------------- MinIO (S3) ----------------
  minio:
    image: quay.io/minio/minio:RELEASE.2024-06-13T22-53-53Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------- Redpanda (Kafka API) ----------------
  redpanda:
    image: redpandadata/redpanda:v23.3.10
    container_name: redpanda
    command:
      - redpanda start
      - --overprovisioned
      - --smp=1
      - --memory=1G
      - --reserve-memory=0M
      - --node-id=0
      - --check=false
    ports:
      - "9092:9092"
      - "9644:9644"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9644/v1/status/ready"]
      interval: 5s
      timeout: 5s
      retries: 10

  redpanda-console:
    image: redpandadata/console:v2.4.5
    container_name: redpanda-console
    environment:
      - KAFKA_BROKERS=redpanda:9092
    ports:
      - "8081:8080"
    depends_on:
      - redpanda

  # ---------------- Airflow ----------------
  airflow-db:
    image: postgres:13
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - airflow_db_data:/var/lib/postgresql/data

  airflow:
    build:
      context: ./airflow
    container_name: airflow
    restart: unless-stopped
    depends_on:
      - airflow-db
      - minio
      - redpanda
    environment:
      AIRFLOW__CORE__EXECUTOR: "${AIRFLOW__CORE__EXECUTOR}"
      AIRFLOW__CORE__LOAD_EXAMPLES: "${AIRFLOW__CORE__LOAD_EXAMPLES}"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "${AIRFLOW__CORE__DEFAULT_TIMEZONE}"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: ""
      TZ: ${TZ}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_BUCKET: ${S3_BUCKET}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_TICKS_TOPIC: ${KAFKA_TICKS_TOPIC}
      KAFKA_BARS_TOPIC: ${KAFKA_BARS_TOPIC}
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow-init
    entrypoint: /bin/bash
    command: -c "airflow db migrate && airflow users create --username ${AIRFLOW_ADMIN_USER} --firstname Admin --lastname User --role Admin --email ${AIRFLOW_ADMIN_EMAIL} --password ${AIRFLOW_ADMIN_PASSWORD} && python /init/init_objects.py"
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      S3_BUCKET: ${S3_BUCKET}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_TICKS_TOPIC: ${KAFKA_TICKS_TOPIC}
      KAFKA_BARS_TOPIC: ${KAFKA_BARS_TOPIC}
    volumes:
      - ./init:/init
    depends_on:
      - airflow-db
      - minio
      - redpanda

  # ---------------- Flink ----------------
  flink-jobmanager:
    build:
      context: ./docker/flink-python
    container_name: flink-jobmanager
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    ports:
      - "8088:8088"
    depends_on:
      - redpanda

  flink-taskmanager:
    build:
      context: ./docker/flink-python
    container_name: flink-taskmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    depends_on:
      - flink-jobmanager

  flink-submit:
    build:
      context: ./docker/flink-python
    container_name: flink-submit
    entrypoint: /bin/bash
    working_dir: /opt/flink-app
    environment:
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_TICKS_TOPIC: ${KAFKA_TICKS_TOPIC}
      KAFKA_BARS_TOPIC: ${KAFKA_BARS_TOPIC}
    volumes:
      - ./flink/jobs:/opt/flink-app
    depends_on:
      - flink-jobmanager
      - flink-taskmanager
    command: -lc "python -m pyflink.datastream -py stream_features.py --brokers ${KAFKA_BROKERS} --in-topic ${KAFKA_TICKS_TOPIC} --out-topic ${KAFKA_BARS_TOPIC}"

  # ---------------- Tick Producer (toy) ----------------
  tick-producer:
    image: python:3.11-slim
    working_dir: /app
    volumes:
      - ./scripts:/app
    environment:
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_TICKS_TOPIC: ${KAFKA_TICKS_TOPIC}
    entrypoint: bash -lc "pip install --no-cache-dir kafka-python==2.0.2 pandas==2.2.2 numpy==1.26.4 && python seed_ticks.py"

volumes:
  minio_data:
  airflow_db_data:
